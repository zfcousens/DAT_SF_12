{
 "metadata": {
  "name": "",
  "signature": "sha256:1a4e1f355b759ddad650f63a0ac6bf73a10c240cdd171d207e16bee349d71891"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prework:\n",
      "=======\n",
      "Go to http://spark.apache.org/downloads.html \n",
      "\n",
      "Download Spark: spark-1.2.0.tgz\n",
      "\n",
      "Move it somewhere where you won\u2019t lose it and decompress.\n",
      "\n",
      "\n",
      "Follow the instructions at https://spark.apache.org/docs/latest/ec2-scripts.html:\n",
      "```bash\n",
      "export AWS_ACCESS_KEY_ID=AWS_ACCESS_KEY_ID\n",
      "export AWS_SECRET_ACCESS_KEY=AWS_SECRET_ACCESS_KEY\n",
      "\n",
      "./spark-ec2 launch -k key -i KEY_LOCATION -s 5 \u2014t m3.xlarge my_cluster --copy-aws-credentials\n",
      "./spark-ec2 -i KEY_LOCATION login test_cluster\n",
      "```\n",
      "\n",
      "*N.B. be sure to include* **`--copy-aws-credentials`** *when launching your cluster this time!*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once it's up:\n",
      "```bash\n",
      "./spark-ec2 -i KEY_LOCATION login test_cluster\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Word Count in Spark:\n",
      "-------------------\n",
      "Following example adapted from http://spark.apache.org/docs/1.2.0/quick-start.html:\n",
      "```bash\n",
      "spark/bin/pyspark\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once loaded, import numpy with the following:\n",
      "```python\n",
      "import numpy as np\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "then try:\n",
      "```python\n",
      "gutenberg = sc.textFile(\"s3n://DAT6/gutenberg/\")\n",
      "gutenberg.first()\n",
      "```\n",
      "after a couple dozen log lines, you should see the output:\n",
      "\n",
      "`u'\\ufeffThe Project Gutenberg EBook of The Outline of Science, Vol. 1 (of 4), by '`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try the word count example on these data:\n",
      "```python\n",
      "words = gutenberg.flatMap(lambda line: line.split())\n",
      "word_tuples = words.map(lambda word: (word, 1))\n",
      "word_counts = word_tuples.reduceByKey(lambda a, b: a+b)\n",
      "```\n",
      "PySpark doesn't have an easy way to plot, but we can at least print out the values for a histogram:\n",
      "```python\n",
      "word_counts.values().histogram(map(int, np.logspace(0,6,7)))\n",
      "```\n",
      "We can also look at the top ten most common words by using the second value in the tuple as the sort key:\n",
      "```python\n",
      "word_counts.top(10, key = lambda x: x[1])\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------------\n",
      "Spark SQL in Action:\n",
      "-------------------\n",
      "Following example adapted from http://spark.apache.org/docs/1.2.0/sql-programming-guide.html:\n",
      "```python\n",
      "from datetime import datetime  # we will want the datetime object later\n",
      "from pyspark.sql import SQLContext, Row\n",
      "sqlContext = SQLContext(sc)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the offers csv into an RDD using the following code to split and parse it:\n",
      "```python\n",
      "# Load a text file and convert each line to a dictionary.\n",
      "offers_lines = sc.textFile(\"s3n://dat-sf-12/offers/offers.csv\").filter(lambda x: x[:2] != 'of')\n",
      "offers_parts = offers_lines.map(lambda l: l.split(\",\"))\n",
      "```\n",
      "Test it:\n",
      "```python\n",
      "offers_parts.count()\n",
      "offers_parts.first()\n",
      "```\n",
      "#### Question: why do you suppose we added a filter?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the following to create a SQL table from the RDD:\n",
      "```python\n",
      "offers = offers_parts.map(lambda p: Row(offer = int(p[0]), category = int(p[1]), quantity = int(p[2]), \n",
      "\tcompany = int(p[3]), offervalue = float(p[4]), brand = int(p[5])))\n",
      "\n",
      "# Infer the schema, and register the SchemaRDD as a table.\n",
      "schemaOffers = sqlContext.inferSchema(offers)\n",
      "schemaOffers.registerTempTable(\"offers\")\n",
      "```\n",
      "Test it:\n",
      "```python\n",
      "# SQL can be run over SchemaRDDs that have been registered as a table.\n",
      "sqlContext.sql(\"SELECT * FROM offers LIMIT 10\").collect()\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Pairs:\n",
      "--------\n",
      "\n",
      "#### 1. Do the same with `s3n://dat-sf-12/history/trainHistory.csv`\n",
      "\n",
      "Hint, here's the schema:\n",
      "```python\n",
      "Row(id = int(p[0]), chain = int(p[1]), offer = int(p[2]), market = int(p[3]), \n",
      "\trepeattrips = int(p[4]), repeater = bool(p[5]), offerdate = datetime.strptime(p[6], '%Y-%m-%d'))\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test it:\n",
      "```python\n",
      "recent_offers = sqlContext.sql(\"\"\"SELECT offerdate, id, offervalue \n",
      "    FROM offers JOIN history ON offers.offer = history.offer \n",
      "    ORDER BY offerdate DESC\"\"\")\n",
      "\n",
      "for row in recent_offers.top(10):\n",
      "    '${offervalue:0.2f} off for customer {id} on {offerdate:%m/%d/%Y}'.format(**row.asDict())\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "you should see something like:\n",
      "```\n",
      "'$2.00 off for customer 2118659196 on 04/30/2013'\n",
      "'$2.00 off for customer 2115931556 on 04/30/2013'\n",
      "'$2.00 off for customer 2089678223 on 04/30/2013'\n",
      "'$2.00 off for customer 2073228907 on 04/30/2013'\n",
      "'$2.00 off for customer 2071978226 on 04/30/2013'\n",
      "'$2.00 off for customer 2070014653 on 04/30/2013'\n",
      "'$3.00 off for customer 2056836240 on 04/30/2013'\n",
      "'$2.00 off for customer 2009529994 on 04/30/2013'\n",
      "'$2.00 off for customer 2006719518 on 04/30/2013'\n",
      "'$3.00 off for customer 2006134668 on 04/30/2013'\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2. (optional) Try loading (just) `s3n://dat-sf-12/transactions/trans-aa` and joining it with the above tables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 3. Back to the `gutenberg` data, can you change the function in `.flatMap()` to output *bigrams* (that is, word pairs) instead of single tokens?\n",
      "\n",
      "*e.g.* '`Project Gutenberg EBook of The Outline of Science`' would produce:\n",
      "\n",
      "    ['Project Gutenberg', 'Gutenberg EBook', 'EBook of', 'of The', 'The Outline', 'Outline of', 'of Science']\n",
      "\n",
      "**Then count and sort those bigrams.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 4. (optional) On your own, if you wish, try loading all of `s3n://dat-sf-12/transactions/`. \n",
      "\n",
      "Warning: This takes ~2 hours on 5 m3.xlarge nodes. You may wish to use the [`.sample()`](http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sample) function to speed it up once it's loaded."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Further reading:\n",
      "If you want to learn how to use PySpark in IPython Notebook, look at http://nbviewer.ipython.org/gist/fperez/6384491/00-Setup-IPython-PySpark.ipynb and http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/. It's a little tricky but may be worth the effort if you anticipate doing a lot of data exploration with Spark."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}