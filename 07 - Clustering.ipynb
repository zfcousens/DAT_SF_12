{
 "metadata": {
  "name": "",
  "signature": "sha256:678f7271c06cd3105ac2f6cd0f4eb1494746fddf451e3cfbf39d292f344a0db7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Science\n",
      "================================\n",
      "Clustering\n",
      "--------------------------------\n",
      "Alessandro D. Gagliardi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Clustering\n",
      "================================\n",
      "### Pre-Work\n",
      "\n",
      "- 12 minutes - Independent Research\n",
      "- 6 minutes - Small Group Discussion\n",
      "- ~5 minutes - Class Discussion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "On Your Own\n",
      "================================\n",
      "Say you had access to all of Amazon's or Netflix's customer behavior information and you were tasked to identify types or classes of customers based upon their behavior and/or demographic information. Spend 12 minutes on your own researching how you might do this. You may use the Internet or your own imagination to solve this problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "In Groups of ~4\n",
      "================================\n",
      "Spend 6 minutes discussing your solution with the person next to you. Spend 1-2 minutes each explaining your solution and 1-2 minutes comparing your solutions. One delegate from each group will report findings."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Group Report\n",
      "================================"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Clustering\n",
      "-------------------------\n",
      "2. Hierarchical Clustering\n",
      "3. k-Means Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------\n",
      "Last Time\n",
      "------\n",
      "\n",
      "1. Introduction to Machine Learning\n",
      "2. Classification\n",
      "3. *k-*Nearest Neighbor (*k*NN) Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## What if you don't know your class labels in advance?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/machine_learning7.png\" width=\"800\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Q:  What is a cluster?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  A group of **similar** data points."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The concept of similarity is central to the definition of a cluster, and therefore to cluster analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In general, greater similarity between points leads to better clustering."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Q:  What is the purpose of cluster analysis?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  To enhance our understanding of a dataset by dividing the data into groups."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Clustering provides a layer of *abstraction* from individual data points."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The goal is to extract and enhance the natural structure of the data (not to impose arbitrary structure!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Q:  How do you solve a clustering problem?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  Think of a cluster as a \u201cpotential class\u201d; then the solution to a clustering problem is to programatically determine these classes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The real purpose of clustering is data exploration, so a solution is anything that contributes to your understanding."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Hierarchical Clustering\n",
      "================================"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Single Linkage Hierarchical Clustering\n",
      "--------------------------------------\n",
      "1) Say \"Every point is its own cluster\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "*From [K-means and Hierarchical Clustering](http://www.autonlab.org/tutorials/kmeans.html) by Andrew W. Moore*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "2) Find \u201cmost similar\u201d pair of clusters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "3) Merge it into a parent cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "4) Repeat"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Raw data:\n",
      "![Raw data](http://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hierarchical Representation:\n",
      "![Traditional representation](http://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "*From [Wikipedia](http://en.wikipedia.org/wiki/Hierarchical_clustering)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pros:\n",
      "\n",
      "- Few assumptions (any distance measure will do, no assumptions required about the number of clusters)\n",
      "- Structure of dendogram reveals much about the data\n",
      "\n",
      "### Cons:\n",
      "\n",
      "- Very slow: $O(n^3)$ (especially on big data)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "K-Means Clustering\n",
      "================================"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Q:  What is k-means clustering?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  A **greedy** learner that **partitions** a data set into k clusters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "greedy \u2013 captures local structure (depends on initial conditions)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "partition \u2013 performs complete clustering (each point belongs to exactly one cluster) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Q:  How are these partitions determined?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  Each point is assigned to the cluster with the nearest **centroid**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "centroid \u2013 the mean of the data points in a cluster  \n",
      "$\\rightarrow$ requires continuous (vector-like) features  \n",
      "$\\rightarrow$ highlights iterative nature of algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Centroids & Partitions\n",
      "---------------------------------------\n",
      "Q:  What do these partitions look like?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "![Voronoi Diagrams](http://www.dma.fi.upm.es/mabellanas/tfcs/fvd/images/voronoi.gif)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "These partitions are sometimes called *Voronoi* cells, and these maps *Voronoi diagrams*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Scale Dependance\n",
      "---------------------------------------\n",
      "One important point to keep in mind is that (like kNN) partitions are not scale-invariant!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This means that the same data can yield very different clustering results depending on the scale and the units used."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Therefore it\u2019s important to think about your data representation before applying a clustering algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "![Unscaled](assets/kmeans_before.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Spot the clusters!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "![Scaled](assets/kmeans_after.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "*source: [Data Analysis with Open Source Tools](http://shop.oreilly.com/product/9780596802363.do), by Philipp K. Janert. O\u2019Reilly Media, 2011.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Basic K-means Algorithm\n",
      "---------------------------------------\n",
      "\n",
      "1. Choose $k$ initial centroids (note that $k$ is an input)\n",
      "2. For each point:\n",
      "     - find distance to each centroid\n",
      "     - assign point to nearest centroid\n",
      "3. Recalculate centroid positions\n",
      "4. Repeat steps 2-3 until stopping criteria met\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "*Note that the $k$ in this case is different from the $k$ in k-Nearest Neighbors.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![algorithm](assets/kmeans_3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "centroids converge to (local) optimum"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "*source: http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Strengths & Weaknesses\n",
      "---------------------------------------\n",
      "K-means is algorithmically pretty efficient (time & space complexity is linear in number of records)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "It has a hard time dealing with non-convex clusters, or data with widely varying shapes and densities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Difficulties can sometimes be overcome by increasing the value of $k$ and combining subclusters in a post-processing step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Note: alternative for categorical features = k-medoids\n",
      "(medoid is analog of centroid, but must itself be a member of the dataset)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Step 1 - Choosing Initial Centroids\n",
      "---------------------------------------\n",
      "Q:  How do you choose the initial centroid positions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Note: hierarchical clustering is comp expensive (in this case, should sample data), but does not require initial choice of centroids"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  There are several options:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- randomly (but may yield divergent behavior)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Most general technique, good place to start (div behavior is always possible in a greedy scheme)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- perform alternative clustering task, use resulting centroids as initial k-means centroids"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- start with global centroid, choose point at max distance, repeat (but might select outlier)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Dealing with outliers in clustering is another topic (remove, don\u2019t remove, detection)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Step 2 - Similarity Measures\n",
      "---------------------------------------\n",
      "Q:  How do you determine which centroid is the nearest?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The \u201cnearness\u201d criterion is determined by the similarity/distance measure we discussed earlier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This measure makes quantitative inference possible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Technically, by defining a similarity measure we are mapping our observations into a *metric space*.  \n",
      "Note: numbers are already in metric space, but strings, genes, etc. are not"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The matrix whose entries $D_{ij}$ contain the values $d(x, y)$ for all $x$ and $y$ is called the distance matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The distance matrix contains *all of the information* we know about the dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For this reason, it\u2019s really the choice of metric that determines the definition of a cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Step 3 - Objective Function\n",
      "---------------------------------------\n",
      "Q:  How do we recompute the positions of the centroids at each iteration of the algorithm?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A:  By optimizing an **objective function** that tells us how \u201cgood\u201d the clustering is."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The iterative part of the algorithm (recomputing centroids and reassigning points to clusters) explicitly tries to minimize this objective function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Ex: Using the Euclidean distance measure, one typical objective function is the **sum of squared errors** from each point $x$ to its centroid $c_i$:\n",
      "\n",
      "$$ SSE = \\sum\\limits_{i=1}^K\\sum\\limits_{x \\in C_i}d(x_i,c_i)^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Given two clusterings, we will prefer the one with the lower SSE since this means the centroids have converged to better locations (a better local optimum)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Step 4 - Convergence\n",
      "---------------------------------------\n",
      "We iterate until some stopping criteria are met; in general, suitable convergence is achieved in a small number of steps."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Stopping criteria can be based on the centroids (e.g. if positions change by no more than $\\epsilon$) or on the points (e.g. if no more than $x\\%$ change clusters between iterations)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Recall that, in general, different runs of the algorithm will converge to different local optima (centroid configurations)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Questions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "LAB\n",
      "===\n",
      "#### [07 - Clustering Lab](07 - Clustering Lab.ipynb)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}